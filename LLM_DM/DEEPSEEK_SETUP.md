# ä½¿ç”¨ DeepSeek API æŒ‡å—

## ä¸ºä»€ä¹ˆé€‰æ‹© DeepSeekï¼Ÿ

- ğŸ’° **æ€§ä»·æ¯”é«˜**ï¼šæ¯” OpenAI ä¾¿å®œå¾ˆå¤š
- ğŸ‡¨ğŸ‡³ **ä¸­æ–‡å‹å¥½**ï¼šå¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½
- âš¡ **é€Ÿåº¦å¿«**ï¼šå“åº”é€Ÿåº¦è¾ƒå¿«
- ğŸ”Œ **å…¼å®¹æ€§å¥½**ï¼šä½¿ç”¨ OpenAI å…¼å®¹çš„ API æ¥å£

## é…ç½®æ­¥éª¤

### 1. è·å– DeepSeek API Key

1. è®¿é—® [DeepSeek å¼€æ”¾å¹³å°](https://platform.deepseek.com/)
2. æ³¨å†Œè´¦å·å¹¶ç™»å½•
3. è¿›å…¥ API Keys é¡µé¢åˆ›å»ºæ–°çš„ API Key
4. å¤åˆ¶ä½ çš„ API Key

### 2. é…ç½®ç¯å¢ƒå˜é‡

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ DeepSeek API Keyï¼š

```bash
# DeepSeek API Configuration
DEEPSEEK_API_KEY=your-actual-deepseek-api-key-here
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat
```

### 3. æ›´æ–°é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æƒ³é»˜è®¤ä½¿ç”¨ DeepSeekï¼Œç¼–è¾‘ `config/settings.yaml`ï¼š

```yaml
llm:
  provider: "deepseek"  # ä½¿ç”¨ deepseek
  model: "deepseek-chat"
  temperature: 0.3
  max_tokens: 2000
  timeout: 30
```

## ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

å¦‚æœå·²ç»åœ¨ `config/settings.yaml` ä¸­è®¾ç½®äº† `provider: "deepseek"`ï¼Œç›´æ¥è¿è¡Œï¼š

```bash
python src/main.py generate \
  --prompt "ç”Ÿæˆ1000æ¡è¶…å¸‚äº¤æ˜“è®°å½•" \
  --output data/processed/dataset.spmf
```

### æ–¹å¼ 2ï¼šé€šè¿‡ä»£ç æŒ‡å®š

åœ¨ Python ä»£ç ä¸­æ˜ç¡®æŒ‡å®šä½¿ç”¨ DeepSeekï¼š

```python
from llm.client import LLMClient

# ä½¿ç”¨ DeepSeek
client = LLMClient(
    provider="deepseek",
    model="deepseek-chat",
    temperature=0.3
)

config = client.generate_config("ç”Ÿæˆ1000æ¡äº¤æ˜“è®°å½•ï¼Œ100ç§å•†å“")
print(config)
```

### æ–¹å¼ 3ï¼šä¸´æ—¶åˆ‡æ¢

ä½ ä¹Ÿå¯ä»¥ä¿æŒé…ç½®æ–‡ä»¶ä¸å˜ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡ä¸´æ—¶åˆ‡æ¢ï¼š

```bash
# ä¸´æ—¶ä½¿ç”¨ DeepSeek
export LLM_PROVIDER=deepseek
python src/main.py generate --prompt "..." --output data.spmf
```

## DeepSeek æ¨¡å‹é€‰æ‹©

DeepSeek æä¾›å¤šä¸ªæ¨¡å‹ï¼š

| æ¨¡å‹åç§° | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|
| `deepseek-chat` | é€šç”¨å¯¹è¯æ¨¡å‹ | æœ¬é¡¹ç›®æ¨èä½¿ç”¨ |
| `deepseek-coder` | ä»£ç ç”Ÿæˆæ¨¡å‹ | å¦‚æœç”Ÿæˆä»£ç é…ç½® |

## ä»·æ ¼å¯¹æ¯”ï¼ˆä»…ä¾›å‚è€ƒï¼‰

| æä¾›å•† | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ |
|-------|---------|---------|
| OpenAI GPT-4o-mini | $0.15/1M tokens | $0.60/1M tokens |
| DeepSeek Chat | Â¥1/1M tokens | Â¥2/1M tokens |

ğŸ’¡ **DeepSeek çº¦ä¸º OpenAI ä»·æ ¼çš„ 1/10**

## å®Œæ•´ç¤ºä¾‹

### 1. é…ç½® .env

```bash
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat
```

### 2. ç”Ÿæˆæ•°æ®é›†

```bash
python src/main.py generate \
  --prompt "ç”Ÿæˆ2000æ¡é›¶å”®äº¤æ˜“æ•°æ®ï¼ŒåŒ…å«150ç§å•†å“ï¼Œä½¿ç”¨Zipfåˆ†å¸ƒï¼Œæ³¨å…¥3ä¸ªé¢‘ç¹æ¨¡å¼" \
  --output data/processed/retail_deepseek.spmf \
  --stats
```

### 3. è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
python src/main.py full-pipeline \
  --prompt "ç¨€ç–å‹ç”µå•†æ•°æ®ï¼Œ1000ç¬”äº¤æ˜“" \
  --algorithms Apriori FPGrowth \
  --min-support 0.05
```

## æ³¨æ„äº‹é¡¹

1. **JSON æ ¼å¼è¾“å‡º**ï¼šDeepSeek æ”¯æŒ OpenAI çš„ `response_format` å‚æ•°ï¼Œå¯ä»¥å¼ºåˆ¶è¿”å› JSON
2. **é€Ÿç‡é™åˆ¶**ï¼šæ³¨æ„ API çš„é€Ÿç‡é™åˆ¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
3. **å…¼å®¹æ€§**ï¼šç”±äºä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼Œæ‰€æœ‰ OpenAI çš„åŠŸèƒ½éƒ½æ”¯æŒ

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: API Key æ— æ•ˆ

```
ValueError: DEEPSEEK_API_KEY not found in environment variables
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ `.env` æ–‡ä»¶æ˜¯å¦æ­£ç¡®é…ç½®
- ç¡®ä¿ API Key æ²¡æœ‰å¤šä½™çš„ç©ºæ ¼
- é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡

### é—®é¢˜ 2: è¿æ¥å¤±è´¥

```
Exception: LLM API call failed: Connection error
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ç¡®è®¤ `DEEPSEEK_BASE_URL` æ˜¯å¦æ­£ç¡®
- æ£€æŸ¥æ˜¯å¦éœ€è¦ä»£ç†

### é—®é¢˜ 3: å“åº”æ ¼å¼é”™è¯¯

```
ValueError: LLM returned invalid JSON
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- DeepSeek å·²æ”¯æŒ JSON æ¨¡å¼ï¼Œä½†å¶å°”å¯èƒ½å¤±è´¥
- å¯ä»¥é™ä½ `temperature` å‚æ•°ï¼ˆæ›´ç¡®å®šæ€§ï¼‰
- æ£€æŸ¥ Prompt æ˜¯å¦æ¸…æ™°

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **è°ƒæ•´æ¸©åº¦**ï¼š`temperature=0.2` å¯ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
2. **å‡å°‘ tokens**ï¼š`max_tokens=1500` å¯¹äºé…ç½®ç”Ÿæˆå·²è¶³å¤Ÿ
3. **æ‰¹é‡å¤„ç†**ï¼šå¦‚æœéœ€è¦ç”Ÿæˆå¤šä¸ªæ•°æ®é›†ï¼Œå¯ä»¥æ‰¹é‡è¯·æ±‚

## ä¸ OpenAI åˆ‡æ¢

å¦‚æœéœ€è¦åˆ‡æ¢å› OpenAIï¼Œåªéœ€ä¿®æ”¹ `config/settings.yaml`ï¼š

```yaml
llm:
  provider: "openai"
  model: "gpt-4o-mini"
```

æˆ–åœ¨ä»£ç ä¸­æŒ‡å®šï¼š

```python
client = LLMClient(provider="openai")
```

---

**Happy Data Mining with DeepSeek! ğŸš€**
